# train_xgboost.py
import pandas as pd
import numpy as np
import joblib
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import os

def train_model():
    input_path = "data/processed/features.csv"
    if not os.path.exists(input_path):
        raise FileNotFoundError(f"âš ï¸  Archivo no encontrado: {input_path}. Ejecuta primero prepare_dataset.py")
    
    df = pd.read_csv(input_path)
    print(f"ğŸ“¥ Cargados {len(df)} partidos para entrenamiento.")
    
    # âœ… Features SIN cuotas (solo estadÃ­sticas histÃ³ricas)
    feature_cols = [
        'home_goals_avg', 'home_conceded_avg', 'home_win_rate',
        'away_goals_avg', 'away_conceded_avg', 'away_win_rate'
    ]
    
    # VerificaciÃ³n adicional
    missing_cols = [col for col in feature_cols if col not in df.columns]
    if missing_cols:
        raise KeyError(f"Las siguientes columnas faltan en el dataset: {missing_cols}")
    
    X = df[feature_cols]
    y = df['target']  # 0=Local, 1=Empate, 2=Visitante
    
    print(f"ğŸ“Š Features utilizadas: {feature_cols}")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print("ğŸš€ Entrenando modelo XGBoost con regularizaciÃ³n...")
    model = XGBClassifier(
        # ğŸ”‘ ParÃ¡metros de regularizaciÃ³n fuerte
        n_estimators=80,        # Reducido de 200 â†’ menos sobreajuste
        max_depth=3,           # Reducido de 5 â†’ Ã¡rboles mÃ¡s simples
        learning_rate=0.05,    # Reducido de 0.1 â†’ aprendizaje mÃ¡s lento = mÃ¡s robusto
        gamma=0.5,             # âœ… MÃ­nima pÃ©rdida para hacer split (0.5+)
        min_child_weight=5,    # âœ… MÃ­nimo peso de instancias en nodo hijo (3+)
        reg_alpha=1.0,         # âœ… RegularizaciÃ³n L1 (1.0+)
        reg_lambda=1.5,        # âœ… RegularizaciÃ³n L2 (1.0+)
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        objective='multi:softmax',
        num_class=3,
        eval_metric='mlogloss'
    )
    
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
    
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\nâœ… Resultados del modelo:")
    print(f"   PrecisiÃ³n general: {accuracy:.2%}")
    print(f"\n{classification_report(y_test, y_pred, target_names=['Local', 'Empate', 'Visitante'])}")
    
    os.makedirs("models", exist_ok=True)
    model_path = "models/xgb_model.pkl"
    joblib.dump(model, model_path)
    print(f"ğŸ’¾ Modelo guardado en: {model_path}")
    
    importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    importance.to_csv("models/feature_importance.csv", index=False)
    print(f"ğŸ“ˆ Importancia de features guardada en: models/feature_importance.csv")
    print(importance)

if __name__ == "__main__":
    train_model()